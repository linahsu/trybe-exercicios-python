# Selenium

Essencialmente, o Selenium √© um conjunto de ferramentas para automa√ß√£o de navegadores da web, que permite controlar remotamente inst√¢ncias de navegadores e emular a intera√ß√£o de um usu√°rio com o navegador.

No n√∫cleo do Selenium est√° o WebDriver, uma API e protocolo que define uma interface para controlar o comportamento dos navegadores web. √â atrav√©s do WebDriver que o Selenium suporta a automa√ß√£o dos principais navegadores do mercado.

Cada navegador possui uma implementa√ß√£o espec√≠fica do WebDriver, chamada de driver, que √© o componente respons√°vel por delegar e manipular a comunica√ß√£o entre o Selenium e o navegador.

Para utilizar o Selenium √© necess√°rio instalar as bibliotecas de acordo com a linguagem que voc√™ utilizar√°, al√©m de ter instalado o navegador que deseja usar e o driver correspondente para ele.

Ap√≥s essas instala√ß√µes, com apenas algumas linhas de c√≥digo podemos criar uma inst√¢ncia de navegador para simular um acesso real a um site, como uma pessoa utilizando o browser faria.

Essa fun√ß√£o pode ser extremamente √∫til quando falamos de raspagem de dados, pois em Single Page Applications, por exemplo, como sites constru√≠dos em React, s√≥ ap√≥s uma requisi√ß√£o para a p√°gina √© que o conte√∫do ser√° montado e as informa√ß√µes estar√£o dispon√≠veis para serem consultadas. Simulando um acesso de uma pessoa real ao site, o Selenium pode evitar que o resultado da consulta volte vazio nesses casos.

Por exemplo, se voc√™ usar a biblioteca requests para fazer uma requisi√ß√£o do tipo get em um site feito em React, voc√™ vai receber de volta um HTML relativamente simples, somente com um elemento root e um c√≥digo JavaScript grande, que seria o respons√°vel por criar dinamicamente a p√°gina web. Acontece que esse c√≥digo JavaScript n√£o √© rodado pelo Python, permanecendo apenas um texto, fazendo com que sua p√°gina nunca seja montada e que voc√™ n√£o ache as informa√ß√µes que desejava. J√° o Selenium cria uma inst√¢ncia de um navegador, por exemplo o Firefox, e de fato executa o JavaScript como se fosse uma pessoa acessando o site pelo navegador normalmente, e somente ap√≥s o site terminar de carregar √© que o scrape √© feito.

## Instala√ß√£o do Selenium

Podemos utilizar o Selenium tanto instalado diretamente em nossa m√°quina, quanto atrav√©s de um container Docker. A seguir teremos o passo a passo das duas formas de instala√ß√£o da ferramenta, mas voc√™ precisa seguir apenas uma delas, a escolha fica inteiramente a seu crit√©rio, para conseguir executar o exemplo desenvolvido ao longo do conte√∫do.

Independentemente da forma escolhida, lembre-se de criar um ambiente virtual antes de instalar bibliotecas.

<details>
<summary><strong> Instala√ß√£o utilizando o Docker </strong></summary>

Ao optar por essa instala√ß√£o, √© essencial ter o Docker instalado em seu computador. Para consultar como fazer a instala√ß√£o voc√™ pode acessar este conte√∫do.

A imagem que utilizaremos do Selenium √© a selenium/standalone-firefox:

```bash
docker pull selenium/standalone-firefox:106.0
```

### Em computadores MacOS com o chip M1

A imagem Docker recomendada h√° pouco pode n√£o funcionar corretamente em computadores MacOS com o chip M1. Se este for o caso da sua m√°quina, voc√™ pode utilizar a imagem seleniarm/standalone-firefox:

```bash
docker pull seleniarm/standalone-firefox:105.0
```

### Iniciando a imagem Docker

Para efetivamente come√ßar a utilizar o Selenium, precisaremos inicializar a imagem Docker que baixamos anteriormente. Na documenta√ß√£o da imagem recomendada anteriormente √†s pessoas que utilizam Linux ou MacOS sem o novo processador M1, especificamente na se√ß√£o Using your browser (no VNC client is needed), que permite a inspe√ß√£o visual da atividade do container atrav√©s do navegador, encontramos os comandos que utilizaremos a seguir.

Faremos uma √∫nica adapta√ß√£o, que √© acrescentar a flag --name para nomear o container e facilitar sua manipula√ß√£o.

```bash
docker run -d -p 4444:4444 -p 7900:7900 --shm-size 2g --name firefox selenium/standalone-firefox:106.0
```

As flags utilizadas acima t√™m as seguintes finalidades:

* -d serve para rodar o container em segundo plano
* -p vincula uma porta do SO a outra porta dentro do container
* --shm-size aumenta o limite de quantidade de mem√≥ria compartilhada com o container
* --name define qual vai ser o nome do container
* --platform (somente se utilizado) diz ao docker qual a plataforma deve ser usada

Pessoas que utilizam MacOS com o chip M1, podem consultar a documenta√ß√£o da imagem recomendada aqui. Neste caso, o comando a ser utilizado ser√° levemente diferente do apresentado h√° pouco:

```bash
docker run -d --name firefox -p 4444:4444 -p 7900:7900 --shm-size 2g seleniarm/standalone-firefox:105.0
```

Acessando o navegador Firefox na porta 7900, poderemos conferir se o container est√° rodando corretamente. Conforme a documenta√ß√£o, ser√° necess√°rio apenas utilizar a senha secret para ter acesso ao container.

‚ö†Ô∏è Aten√ß√£o: Caso aconte√ßa de o container travar, basta que voc√™ o reinicie, com o comando:

```bash
docker restart firefox
```

</details>
</br>

<details>
<summary><strong> Instala√ß√£o local </strong></summary>

Para fazer a instala√ß√£o do Selenium diretamente em sua m√°quina, basta executar o comando a seguir em seu terminal:

```bash
pip3 install selenium
```

Neste exemplo, vamos utilizar a vers√£o 4.6.0, que pode ser instalada com o seguinte comando:

```bash
pip3 install selenium==4.6.0
```

Como dito anteriormente, para utilizar a ferramenta √© necess√°rio tamb√©m instalar o driver do browser que utilizaremos. Nesta parte da documenta√ß√£o (https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location/) voc√™ encontra os links para os drivers dos principais browsers. Para este exemplo utilizaremos o Mozilla Firefox (especificamente na vers√£o 106.0), que j√° vem instalado na maioria das distribui√ß√µes Linux, mas a utiliza√ß√£o de outros navegadores, como o Chrome, √© bem similar.

Ap√≥s escolher o browser que ser√° utilizado, clique em Downloads, e depois do redirecionamento para a p√°gina do github com as op√ß√µes do geckodriver, clique na vers√£o compat√≠vel com o sistema operacional instalado em seu computador.

‚ö†Ô∏è Caso v√° utilizar o Chrome, o link de Download redirecionar√° para uma p√°gina com diversas vers√µes do driver. Neste caso, clique na op√ß√£o compat√≠vel com a vers√£o do navegador que voc√™ tem instalada e em seguida escolha a vers√£o compat√≠vel com seu sistema operacional. Para consultar a vers√£o do Chrome, basta ir em seu navegador, nos tr√™s pontinhos na parte superior direita da barra de tarefas, clicar em ‚ÄòAjuda‚Äô (help) e em seguida em ‚ÄòSobre o Google Chrome‚Äô (About Google Chrome).

‚ö†Ô∏è Caso esteja fazendo o download via linha de comando, voc√™ pode utilizar o utilit√°rio wget. Para baixar o geckodriver-v0.32.0 para o Firefox, o comando √© wget https://github.com/mozilla/geckodriver/releases/download/v0.32.0/geckodriver-v0.32.0-linux64.tar.gz (observe que esta √© a vers√£o para Linux). Para extrair o driver voc√™ pode utilizar utilit√°rio tar. Outras vers√µes do geckodriver para outros sistemas operacionais podem ser encontradas aqui. ‚ö†Ô∏è Aten√ß√£o: √© essencial que tanto o driver quanto o navegador estejam no path.**

Nas distros Linux e no MacOS, o pr√≥ximo passo √© extrair o arquivo baixado e mov√™-lo para o diret√≥rio ‚Äò/usr/bin‚Äô. Para isso, abra no terminal o diret√≥rio onde est√° o arquivo rec√©m baixado e utilize o comando:

```bash
sudo mv geckodriver /usr/bin
```

Caso voc√™ esteja em um ambiente virtual, o diret√≥rio bin do ambiente √© adicionado ao path automaticamente, ent√£o voc√™ pode mover o bin√°rio para l√°:

```bash
mv geckodriver .venv/bin/
```

Al√©m disso, voc√™ pode simplesmente copiar e colar ou at√© mesmo arrastar e soltar o arquivo geckodriver dentro do diret√≥rio /bin em seu ambiente virtual.
</details>
</br>

<details>
<summary><strong> Primeiros passos com o Selenium </strong></summary>

Vamos criar um arquivo para fazer nossos primeiros experimentos com o Selenium! üöÄ

üëÄ De olho na dica: √© importante evitar utilizar os nomes das bibliotecas e ferramentas para nomear os arquivos.

‚ö†Ô∏è Para que o c√≥digo do exemplo funcione, lembre-se que √© necess√°rio ter o Firefox instalado no seu computador.

Agora vamos colocar a m√£o na massa! Crie o seguinte arquivo:

selenium_example.py

```bash
# importa√ß√£o do webdriver, que √© o que possibilita a implementa√ß√£o para todos
# os principais navegadores da web
from selenium import webdriver

# cria√ß√£o de uma inst√¢ncia de navegador utilizando o Firefox
firefox = webdriver.Firefox()

# requisi√ß√µes para essa inst√¢ncia criada utilizando o m√©todo `get`
response = firefox.get("https://www.python.org/")
```

Caso voc√™ esteja utilizando o Selenium com Docker, seu c√≥digo inicial ser√° um pouco diferente, pois precisamos passar algumas op√ß√µes e utilizar o m√©todo remote para vincular nosso arquivo de c√≥digo ao container rodando na porta 7700. Portanto, seu c√≥digo inicial ficar√° assim:

selenium_example.py

```bash
# importa√ß√£o do webdriver, que √© o que possibilita a implementa√ß√£o para todos
# os principais navegadores da web
from time import sleep
from selenium import webdriver

# Para usar o chrome ao inv√©s do firefox trocamos FirefoxOptions por ChromeOptions
# Todavia, caso esteja utilizando o docker, atente-se ao container sendo utilizado.
options = webdriver.FirefoxOptions()
options.add_argument('--ignore-certificate-errors')
options.add_argument('--ignore-ssl-errors=yes')
options.add_argument('--start-maximized')

firefox = webdriver.Remote(command_executor="http://localhost:4444/wd/hub", options=options)

# requisi√ß√µes para essa inst√¢ncia criada utilizando o m√©todo `get`
response = firefox.get("https://www.python.org/")

# espera 10 segundos
sleep(10)

# encerra o navegador, importante quando usamos containers
firefox.quit()
```

Daqui para frente, utilizaremos como base o c√≥digo em que a instala√ß√£o foi feita diretamente na m√°quina, por isso tenha aten√ß√£o nas linhas que trazem as options e a que faz a defini√ß√£o de firefox, pois elas n√£o poder√£o ser removidas no caso de voc√™ estar utilizando Docker. O restante do c√≥digo pode seguir as instru√ß√µes dos pr√≥ximos passos normalmente.

Executando o c√≥digo acima, voc√™ ver√° que uma janela do navegador abrir√° automagicamente no site solicitado. Se voc√™ reparar, um √≠cone aparece na barra de endere√ßo do navegador e se voc√™ passar o mouse por cima ver√° a mensagem ‚ÄúBrowser is under remote control (reason: Marionette)‚Äù.

Caso voc√™ esteja utilizando o Selenium com Docker, todas as a√ß√µes executadas ser√£o vistas na janela do Firefox no endere√ßo http://localhost:7900.

‚ö†Ô∏è A partir deste momento, o ideal √© que √© voc√™ abra os endpoints dos exemplos e inspecione as p√°ginas para entender como a p√°gina est√° estruturada e compreender melhor porque estamos pegando cada informa√ß√£o daquela maneira. Al√©m de ajudar no aprendizado, isso √© importante porque estamos utilizando p√°ginas reais da web, que est√£o recebendo manuten√ß√£o e atualiza√ß√µes constantes. Isso significa que um campo utilizado no exemplo e que existe na p√°gina hoje, pode n√£o existir mais no site daqui a um m√™s. Trabalhando com web scraping, essa aten√ß√£o √© essencial! üòâ

Voc√™ pode instruir o navegador a realizar diversas opera√ß√µes atrav√©s do c√≥digo. Para dar um exemplo e evidenciar o potencial do Selenium, com apenas tr√™s linhas conseguimos fazer com que logo ap√≥s abrir o navegador, algo seja digitado no campo de pesquisa e o selenium pressione enter para efetivar a busca:

```bash
# from selenium import webdriver
from selenium.webdriver.common.keys import Keys # Importa teclas comuns

# firefox = webdriver.Firefox()

response = firefox.get("https://www.google.com.br/")

# pesquisa na inst√¢ncia aberta do navegador pela primeira ocorr√™ncia
# do nome 'q'
search_input = firefox.find_element("name", "q")

# escreve selenium dentro do campo de pesquisa
search_input.send_keys("selenium")

# pressiona enter para realizar a busca
search_input.send_keys(Keys.ENTER)
```

Partindo para a parte que nos interessa, a de web scraping, vamos juntar os conhecimentos da √∫ltima aula ao que j√° vimos hoje e pegar algumas informa√ß√µes dos livros de uma p√°gina dedicada para treinar scraping.

O Selenium tem v√°rios m√©todos p√∫blicos, como o find_element que usamos anteriormente e o find_elements (no plural), utilizados para localizar o primeiro elemento correspondente ao resultado do filtro ou todos os elementos que se encaixarem na busca, respectivamente.

Tamb√©m podemos utilizar o By para especificar um elemento CSS ou XPATH que ser√° buscado, para isso √© necess√°rio import√°-lo via:

```bash
from selenium.webdriver.common.by import By
```

Nesse caso, devemos passar dois par√¢metros: o primeiro par√¢metro define o que voc√™ ir√° buscar e o segundo o filtro da nossa pesquisa. Abaixo temos duas op√ß√µes no que diz respeito ao que estamos buscando, uma delas utilizando o By e a outra no formato previamente utilizado (quando buscamos pelo campo de nome q no exemplo acima).

```bash
Com o By	            Sem o By
By.ID	                ‚Äúid‚Äù
By.NAME	                ‚Äúname‚Äù
By.XPATH	            ‚Äúxpath‚Äù
By.LINK_TEXT	        ‚Äúlink text‚Äù
By.PARTIAL_LINK_TEXT	‚Äúpartial link text‚Äù
By.TAG_NAME         	‚Äútag name‚Äù
By.CLASS_NAME	        ‚Äúclass name‚Äù
By.CSS_SELECTOR	        ‚Äúcss selector‚Äù
```

Assim como quando utilizamos a lib requests, inspecionar a p√°gina que queremos raspar √© imprescind√≠vel para entendermos a estrutura de como a p√°gina foi montada e quais elementos devemos selecionar para buscar as informa√ß√µes que queremos.

scrape_selenium.py

```bash
from selenium import webdriver

# Importa o By
from selenium.webdriver.common.by import By

firefox = webdriver.Firefox()

firefox.get("https://books.toscrape.com/")


# Define a fun√ß√£o que far√° o scrape da URL recebida
def scrape(url):
    firefox.get(url)

    # Itera entre os elementos com essa classe
    for book in firefox.find_elements(By.CLASS_NAME, "product_pod"):
        # Cria dict vazio para guardar os elementos capturados
        new_item = {}

        # Cria uma chave 'title' no dict que vai receber o resultado da busca
        # O t√≠tulo est√° em uma tag anchor que est√° dentro de uma tag 'H3'
        new_item["title"] = (
            book.find_element(By.TAG_NAME, "h3")
            .find_element(By.TAG_NAME, "a")
            .get_attribute("innerHTML")
        )

        # O pre√ßo do book est√° em um elemento da classe 'price_color'
        new_item["price"] = book.find_element(
            By.CLASS_NAME, "price_color"
        ).get_attribute("innerHTML")

        # O link est√° dentro de um atributo 'href'
        new_item["link"] = (
            book.find_element(By.CLASS_NAME, "image_container")
            .find_element(By.TAG_NAME, "a")
            .get_attribute("href")
        )

        print(new_item)


scrape("https://books.toscrape.com/")
```

Utilizamos bastante no c√≥digo acima o m√©todo get_attribute. A raz√£o disso √© que com o Selenium, o elemento retornado depois da busca pelo atributo CSS ser√° do tipo webdriver e n√£o texto. √â justamente para fazer essa convers√£o que utilizamos esse m√©todo especificando o atributo ‚ÄòinnerHTML‚Äô ou ‚Äòhref‚Äô, por exemplo.

Tamb√©m utilizamos o m√©todo find_element encadeado para procurar um elemento dentro de outro elemento, como fizemos para pegar o link, por exemplo, que era um elemento √¢ncora dentro de uma div.

Certo, mas at√© agora s√≥ pegamos informa√ß√µes dos livros da primeira p√°gina do site, como fazemos para pegar todos os livros, at√© a √∫ltima p√°gina? ü§î

1. Primeiro precisamos organizar nosso c√≥digo e determinar que o retorno da fun√ß√£o scrape salve o resultado da raspagem em uma lista.
2. Em seguida, temos que criar uma nova lista para abrigar os dados de uma p√°gina;
3. Depois devemos acessar o bot√£o para ir para a pr√≥xima p√°gina e l√° refazer o processo de salvar todas as informa√ß√µes solicitadas, repetindo esse mecanismo at√© todas as p√°ginas serem percorridas.

Vamos ver como fazer isso com Selenium?

```bash
# from selenium import webdriver

# from selenium.webdriver.common.by import By


# firefox = webdriver.Firefox()


# def scrape(url):
#     firefox.get(url)

    books = []

#     for book in firefox.find_elements(By.CLASS_NAME, "product_pod"):
#         new_item = {}

#         new_item["title"] = (
#             book.find_element(By.TAG_NAME, "h3")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("innerHTML")
#         )

#         new_item["price"] = book.find_element(
#             By.CLASS_NAME, "price_color"
#         ).get_attribute("innerHTML")

#         new_item["link"] = (
#             book.find_element(By.CLASS_NAME, "image_container")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("href")
#         )

        books.append(new_item)
    return books

firefox.get("https://books.toscrape.com/")

all_books = []
url2request = "https://books.toscrape.com/"

# Cria uma vari√°vel com o seletor que captura o link do bot√£o de passar para
# a pr√≥xima p√°gina
next_page_link = (
    firefox.find_element(By.CLASS_NAME, "next")
    .get_attribute("innerHTML")
)

# Enquanto este bot√£o de 'next' existir na p√°gina ele ir√° iterar
while next_page_link:

    # Usa o m√©todo extend para colocar todos os elementos de uma lista dentro
    # de outra
    all_books.extend(scrape(url2request))
    url2request = (
        firefox.find_element(By.CLASS_NAME, "next")
        .find_element(By.TAG_NAME, "a")
        .get_attribute("href")
    )

print(all_books)
```

Como h√° muitas p√°ginas a serem percorridas, √© normal que a execu√ß√£o deste c√≥digo leve alguns minutos. üòâ

Observando o navegador aberto com a execu√ß√£o do c√≥digo, voc√™ ver√° que ele abriu na p√°gina solicitada e em seguida come√ßou a percorrer as p√°ginas do site, o que indica que tudo est√° correndo bem. Contudo, pouco antes de finalizar, j√° na √∫ltima p√°gina, no terminal aparece uma exce√ß√£o do Selenium chamada ‚ÄòNoSuchElementException‚Äô.

Pela mensagem fica mais f√°cil de entender o que deu errado. Ao entrar na √∫ltima p√°gina, ele fez a raspagem de todas as informa√ß√µes pedidas, por√©m o passo seguinte foi imposs√≠vel de executar j√° que precisava achar o bot√£o ‚Äònext‚Äô para alterar o link na vari√°vel url2request. A forma mais simples de resolver este erro √© importar a exce√ß√£o do Selenium e trat√°-la com um try, de forma que ao ocorrer esta situa√ß√£o o loop seja quebrado.

```bash
# from selenium import webdriver

# from selenium.webdriver.common.by import By

# from selenium.common.exceptions import NoSuchElementException

# firefox = webdriver.Firefox()


# def scrape(url):
#     firefox.get(url)

#     books = []

#     for book in firefox.find_elements(By.CLASS_NAME, "product_pod"):
#         new_item = {}

#         new_item["title"] = (
#             book.find_element(By.TAG_NAME, "h3")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("innerHTML")
#         )

#         new_item["price"] = book.find_element(
#             By.CLASS_NAME, "price_color"
#         ).get_attribute("innerHTML")

#         new_item["link"] = (
#             book.find_element(By.CLASS_NAME, "image_container")
#             .find_element(By.TAG_NAME, "a")
#             .get_attribute("href")
#         )

#         books.append(new_item)
#     return books


# firefox.get("https://books.toscrape.com/")

# all_books = []
# url2request = "https://books.toscrape.com/"

# next_page_link = (
#     firefox.find_element(By.CLASS_NAME, "next")
#     .get_attribute("innerHTML")
# )

# while next_page_link:

#     all_books.extend(scrape(url2request))
    try:
        url2request = (
            firefox.find_element(By.CLASS_NAME, 'next')
            .find_element(By.TAG_NAME, 'a').get_attribute('href')
        )
    except NoSuchElementException:
        print("exception handled")
        break

# print(all_books)
```

Agora sim, ap√≥s uma eternidade percorrer todas as p√°ginas do site temos as informa√ß√µes que queremos de todos os 1000 livros l√° existentes! ü•≥

Antes de partirmos para a pr√≥xima ferramenta que veremos hoje, aqui vai uma √∫ltima dica: √© muito legal ver o navegador abrir e executar os comandos que definimos, por√©m quando n√£o precisamos ou n√£o queremos ver essa execu√ß√£o, podemos evit√°-la utilizando as options. Basta import√°-las do webdriver, adicionar um novo argumento com a op√ß√£o que deseja e depois pass√°-la como par√¢metro para a inst√¢ncia de navegador criada.

```bash
from selenium import webdriver
# Importa a classe 'Options' do browser
from selenium.webdriver.firefox.options import Options


firefox = webdriver.Firefox()

# Instancia um objeto da classe 'Options'
options = Options()
# Adiciona um argumento passando o par√¢metro '--headless'
options.add_argument('--headless')

# Define que a inst√¢ncia do navegador deve usar as options definidas
firefox = webdriver.Firefox(options=options)

# firefox.get('https://books.toscrape.com/')

# ...
```

üí° Caso queira explorar outras possibilidades oferecidas pelas options, voc√™ pode consultar este link da documenta√ß√£o. Ele redireciona para as op√ß√µes dispon√≠veis para o Firefox, mas na mesma p√°gina voc√™ encontra menus para consultar sobre outros navegadores.
</details>
</br>

# Beautiful Soup

Beautiful Soup √© uma biblioteca Python para extrair dados de arquivos HTML e XML. Ela √© de grande valia para analisar esses arquivos e fornecer maneiras mais simples de navegar, pesquisar e modificar a √°rvore de an√°lise, podendo economizar v√°rias horas de trabalho.

Veremos como utilizar alguns dos recursos do Beautiful Soup 4, mas antes de analisar as informa√ß√µes precisamos baixar o conte√∫do da p√°gina que queremos utilizar.

<details>
<summary><strong> Primeiros passos </strong></summary>

Com base no que voc√™ j√° sabe sobre web scraping, j√° pode ter uma ideia de ferramentas que podemos usar para fazer a requisi√ß√£o para a p√°gina e baixar seu conte√∫do HTML.

Para nosso exemplo, usaremos a biblioteca requests(especificamente na vers√£o 2.27.1) e faremos uma requisi√ß√£o do tipo get para uma p√°gina de frases.

bsoup_example.py

```bash
import requests

url = "https://quotes.toscrape.com"
page = requests.get(url)
print(page.content)
```

Com o conte√∫do da p√°gina baixado, vamos ao que interessa, ver como utilizar o Beautiful Soup para fazer sua an√°lise!
</details>
</br>

<details>
<summary><strong> Instala√ß√£o das bibliotecas </strong></summary>

‚ö†Ô∏è Lembre-se de estar em um ambiente virtual antes de fazer as instala√ß√µes de bibliotecas.

Para instalar as bibliotecas Beautiful Soup e requests basta digitar em seu terminal:

```bash
pip3 install beautifulsoup4==4.11.1 requests==2.27.1
```

Agora podemos importar a lib em nosso arquivo e come√ßar a explorar as funcionalidades e facilidades que a ferramenta oferece.

```bash
# import requests
from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
html_content = page.text

# Cria uma inst√¢ncia do objeto Beautiful Soup e usa o parser de HTML nativo
# do Python
soup = BeautifulSoup(html_content, "html.parser")

# Utiliza o m√©todo prettify para melhorar a visualiza√ß√£o do conte√∫do
print(soup.prettify())
```

O retorno desse c√≥digo √© o HTML da p√°gina, j√° formatado de uma forma muito amig√°vel para a leitura, n√£o concorda?
</details>
</br>

<details>
<summary><strong> Tipos de objetos do Beautiful Soup </strong></summary>

O Beautiful Soup transforma um documento HTML complexo em uma √°rvore de objetos Python. Os quatro tipos de objetos que podemos lidar s√£o Tag, NavigableString, BeautifulSoup e Comment. Na documenta√ß√£o, que est√° dispon√≠vel inclusive em portugu√™s, existe uma se√ß√£o inteira dedicada aos tipos de objetos, mas destacaremos aqui apenas os dois primeiros.

### Tag
Em suma, um objeto do tipo Tag corresponde a uma tag XML ou HTML do documento original. Toda tag possui um nome acess√≠vel atrav√©s de .name. Por exemplo, quando vemos <header>, ele √© um elemento do tipo tag e o nome dessa tag √© header.

As tags tamb√©m podem ter atributos, como classes, ids e etc. Esses atributos s√£o acess√≠veis considerando tag como um dicion√°rio e como podem receber m√∫ltiplos valores, s√£o apresentados em forma de lista.

```bash
# import requests
# from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
# html_content = page.text

# soup = BeautifulSoup(html_content, "html.parser")


# acessando a tag 'title'
title = soup.title

# retorna o elemento HTML da tag
print(title)

# o tipo de 'title' √© tag
print(type(title))

# o nome de 'title' √© title
print(title.name)

# acessando a tag 'footer'
footer = soup.footer

# acessando o atributo classe da tag footer
print(footer['class'])
```

### NavigableString

Uma string corresponde a um texto dentro de uma tag e esse texto fica armazenado na classe NavigableString.

```bash
# import requests
# from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
# html_content = page.text

# soup = BeautifulSoup(html_content, "html.parser")

# title = soup.title
# footer = soup.footer

# retorna o elemento HTML da tag
print(title)

# Acessando a string de uma tag
print(title.string)

# Verificando o tipo dessa string
print(type(title.string))
```

</details>
</br>

<details>
<summary><strong> Buscando na √°rvore </strong></summary>

Assim como nas outras ferramentas apresentadas at√© aqui, o Beautiful Soup tamb√©m possui dois m√©todos principais para encontrar elementos. Eles s√£o o find() e find_all() e a essa altura voc√™ j√° deve ter presumido que a diferen√ßa b√°sica entre eles √© que o primeiro retorna apenas o primeiro elemento que corresponder ao filtro, enquanto o segundo retorna a lista de todos os elementos que baterem com o filtro.

H√° v√°rias possibilidades de filtros a serem utilizados dentro dos m√©todos descritos acima, de strings e regex, at√© fun√ß√µes, e ler a documenta√ß√£o √© essencial para garantir que voc√™ est√° utilizando o m√©todo mais adequado para buscar os dados que deseja.

Existem algumas informa√ß√µes que s√£o bem comuns de querermos extrair, como os valores das ocorr√™ncias de determinada tag, de um atributo ou mesmo todo o texto da p√°gina.

üëÄ De olho na dica: Ao executar o c√≥digo abaixo, tente executar uma impress√£o por vez, deixando os demais prints comentados enquanto isso, para ter uma melhor visualiza√ß√£o dos retornos. üòâ

```bash
# import requests
# from bs4 import BeautifulSoup

# url = "https://quotes.toscrape.com"
# page = requests.get(url)
# html_content = page.text

# soup = BeautifulSoup(html_content, "html.parser")

# Imprime todas as ocorr√™ncias da tag "p" da p√°gina ou uma lista vazia,
# caso nenhum elemento corresponda a pesquisa
print(soup.find_all("p"))

# Imprime o elemento com o id especificado ou "None",
# caso nenhum elemento corresponda a pesquisa
print(soup.find(id="quote"))

# Imprime todo o texto da p√°gina
print(soup.get_text())

# Imprime todas as "divs" que possuam a classe "quote" ou uma lista vazia,
# caso nenhum elemento corresponda a pesquisa
print(soup.find_all("div", {"class": "quote"}))
```

Por debaixo dos panos, soup.find_all("p") e soup.find_all(name="p") s√£o a mesma coisa, da mesma forma que soup.find(id="quote") √© o mesmo que soup.find(attrs={"id": "quote"}). Isso se deve ao fato de argumentos nomeados diferentes de name, attrs, recursive, string e limit serem todos colocados no dicion√°rio dentro do par√¢metro attrs.

Para dar uma vis√£o geral do que podemos utilizar e da simplicidade de fazer scraping com o Beautiful Soup, vamos fazer algo similar ao que fizemos no exemplo de Selenium e raspar as informa√ß√µes de uma p√°gina de not√≠cias do site Tecmundo.

scrape_bsoup.py

```bash
import requests
from bs4 import BeautifulSoup


# Acessa uma URL e retorna um objeto do Beautiful Soup
def get_html_soup(url):
    page = requests.get(url)
    html_page = page.text

    soup = BeautifulSoup(html_page, "html.parser")
    soup.prettify()
    return soup


# Recebe um objeto soup e retorna informa√ß√µes das not√≠cias de uma p√°gina
def get_page_news(soup):

    # Define uma lista vazia a ser populada com os dados do scraping
    news = []

    # Percorre todos os elementos da tag 'article' com a classe especificada
    for post in soup.find_all(
        "article", {"class": "tec--card tec--card--medium"}
    ):

        # Cria um dicion√°rio para guardar os elementos a cada itera√ß√£o
        item = {}

        # Cria um item chamado tag no dicion√°rio para guardar a tag do post
        # Primeiro pesquisa pela div com a classe espec√≠fica
        # Depois pela tag 'a' dentro dos resultados do primeiro filtro
        # Por fim, traz o resultado da string dentro da tag a
        item["tag"] = post.find("div", {"class": "tec--card__info"}).a.string

        # Mesma l√≥gica da busca anterior
        item["title"] = post.find("h4", {"class": "tec--card__title"}).a.string

        # Parecido com o que foi feito anteriormente, mas dessa vez pega
        # o atributo 'href' dentro da tag 'a'
        item["link"] = post.find("h4", {"class": "tec--card__title"}).a["href"]

        # Mesma l√≥gica da primeira busca, mas trazendo a string dentro da 'div'
        # direto
        item["date"] = post.find(
            "div", {"class": "tec--timestamp__item z--font-semibold"}
        ).string

        # Mesma l√≥gica da busca anterior
        item["time"] = post.find(
            "div", {"class": "z--truncate z-flex-1"}
        ).string

        # Adiciona os itens criado no dicion√°rio √† lista 'news'
        news.append(item)

    return news


print(get_page_news(get_html_soup("https://www.tecmundo.com.br/novidades")))
```

Para fazer a pagina√ß√£o e extrair todas as not√≠cias do site, a l√≥gica √© bem similar a utilizada com o Parsel e o Selenium.

```bash
# import requests
# from bs4 import BeautifulSoup


# # Acessa uma URL e retorna um objeto do Beautiful Soup
# def get_html_soup(url):
#     page = requests.get(url)
#     html_page = page.text

#     soup = BeautifulSoup(html_page, "html.parser")
#     soup.prettify()
#     return soup


# # Recebe um objeto soup e retorna informa√ß√µes das not√≠cias de uma p√°gina
# def get_page_news(soup):

#     news = []

#     for post in soup.find_all(
#         "article", {"class": "tec--card tec--card--medium"}
#     ):

#         item = {}

#         item["tag"] = post.find("div", {"class": "tec--card__info"}).a.string

#         item["title"] = post.find("h4", {"class": "tec--card__title"}).a.string

#         item["link"] = post.find("h4", {"class": "tec--card__title"}).a["href"]

#         item["date"] = post.find(
#             "div", {"class": "tec--timestamp__item z--font-semibold"}
#         ).string

#         item["time"] = post.find(
#             "div", {"class": "z--truncate z-flex-1"}
#         ).string

#         news.append(item)

#     return news


# Recebe um objeto soup e retorna o link que redireciona
# para a p√°gina seguinte, caso ele exista
def get_next_page(soup_page):
    try:

        # Busca pela tag 'a' com as classes espec√≠ficas do link desejado
        return soup_page.find(
            "a",
            {"class": "tec--btn"},
        )["href"]
    except TypeError:
        return None


# Recebe uma URL e retorna uma lista com todas as not√≠cias do site
def scrape_all(url):
    all_news = []

    # Enquanto a pesquisa pelo link que vai para a p√°gina seguinte existir
    while get_next_page(get_html_soup(url)) is not None:

        # Imprime a URL da p√°gina seguinte
        print(get_next_page(get_html_soup(url)))

        # Adiciona os elementos da lista com as not√≠cias de cada
        # p√°gina na lista 'all_news'
        all_news.extend(get_page_news(get_html_soup(url)))

        # define a p√°gina seguinte como URL para a pr√≥xima itera√ß√£o
        url = get_next_page(get_html_soup(url))

    return all_news


# Vamos come√ßar perto das √∫ltimas p√°ginas pra n√£o ter que fazer a requisi√ß√£o
# do site inteiro
print(scrape_all("https://www.tecmundo.com.br/novidades?page=11030"))
```

As duas ferramentas que voc√™ viu hoje te deram mais possibilidades de fazer raspagem de dados e podem ampliar seu ferramental para ir muito al√©m na sua carreira, explorando outras funcionalidades delas, buscando como integr√°-las ou at√© mesmo procurando outras ferramentas que tragam ainda mais simplicidade e praticidade ao que voc√™ precisa fazer.
</details>
</br>
